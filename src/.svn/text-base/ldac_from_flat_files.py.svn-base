
from glob import glob
import sys

from nltk.corpus import brown
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from nltk.corpus import wordnet as wn

def morphy_stem(s):
  first_try = wn.morphy(s)
  if first_try:
    return first_try
  else:
    return s

def create_data(glob_expression, lang="english", doc_limit=-1, delimiter=""):
  from nltk.tokenize.treebank import TreebankWordTokenizer
  tokenizer = TreebankWordTokenizer()

  from nltk.corpus import stopwords
  stop = set(stopwords.words('english'))
  # For 20 newsgroups
  stop.add("subject")
  stop.add("lines")
  stop.add("organization")
  stop.add("re")
  stop.add("msg")
  stop.add("wrote")
  
  from string import ascii_lowercase
  
  docs = {}
  files = glob(glob_expression)
  print("Found %i files" % len(files))
  for ii in files:
    text = open(ii).read().lower()
    if delimiter:
      sections = text.split(delimiter)
    else:
      sections = [text]
            
    if doc_limit > 0 and len(docs) > doc_limit:
      print("Passed doc limit %i" % len(docs))
      break
    print(ii, len(sections))

    for jj in xrange(len(sections)):
      docs["%s-%i" % (ii, jj)] = [x for x in tokenizer.tokenize(sections[jj]) \
                                  if (not x in stop) and \
                                      (len(x) > 2) and \
                                  (min(y in ascii_lowercase for y in x))]
  return docs

# Generates an input file that can be used by DF-LDA
if __name__ == "__main__":
  if len(sys.argv) < 3:
    print("Usage: python ldac_from_nltk.py output_file file_pattern [vocab_size]")
    exit(0)
  output_file = sys.argv[1]
  files = sys.argv[2]

  try:
    vocab_size = int(sys.argv[3])
  except IndexError:
    vocab_size = 2500

  docs = create_data(files)

  f = FreqDist()
  for ii in docs:
    for jj in docs[ii]:
      word = morphy_stem(jj)
      f.inc(word)

  vocab = f.keys()[:vocab_size]

  o = open("%s.voc" % output_file, 'w')
  for jj in vocab:
    o.write("%s\n" % jj)
  o.close()

  vocab = dict((vocab[x], x) for x in xrange(len(vocab)))
  o = open("%s.dat" % output_file, 'w')
  o_filename = open("%s.doc" % output_file, 'w')
  for ii in docs:
    f = FreqDist()
    for jj in docs[ii]:
      word = morphy_stem(jj)
      if jj in vocab:
        f.inc(vocab[word])
    for jj in f:
      o.write("%i:%i " % (jj, f[jj]))
    o.write("\n")
    o_filename.write("%s\n" % ii)
  o.close()
  o_filename.close()
